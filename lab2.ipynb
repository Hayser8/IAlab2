{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Laboratorio 2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Julio Garc√≠a Salas - 22076**\n",
    "#### **Sof√≠a Garc√≠a -22210**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¬øPor qu√© el modelo de Naive Bayes se le considera ‚Äúnaive‚Äù?\n",
    "\n",
    "El modelo de Naive Bayes se considera \"naive\" (ingenuo) porque asume que todas las variables predictoras (features) son independientes entre s√≠, lo cual rara vez es cierto en la pr√°ctica. Esta suposici√≥n simplifica enormemente los c√°lculos, permitiendo una clasificaci√≥n r√°pida y eficiente, aunque en muchos casos la independencia condicional no se cumple completamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machine (SVM)\n",
    "   - Explique la formulaci√≥n matem√°tica que se busca optimizar en Support Vector Machine.\n",
    "      El objetivo de **SVM** es encontrar un hiperplano que maximice la **margen** entre dos clases de datos.  \n",
    "      La formulaci√≥n matem√°tica de SVM se basa en minimizar la siguiente funci√≥n objetivo:\n",
    "\n",
    "      $$\n",
    "      \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "      $$\n",
    "\n",
    "      ### Sujeto a las restricciones:\n",
    "\n",
    "      $$\n",
    "      y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "      $$\n",
    "\n",
    "      ### Donde:\n",
    "\n",
    "      **Vector de pesos:**  \n",
    "      $$\n",
    "      \\mathbf{w}\n",
    "      $$\n",
    "\n",
    "      **Sesgo (*bias*):**  \n",
    "      $$\n",
    "      b\n",
    "      $$\n",
    "\n",
    "      **Cada punto de datos de entrenamiento:**  \n",
    "      $$\n",
    "      \\mathbf{x}_i\n",
    "      $$\n",
    "\n",
    "      **Etiqueta de la clase:**  \n",
    "      $$\n",
    "      y_i \\in \\{+1, -1\\}\n",
    "      $$\n",
    "      \n",
    "\n",
    "      Si los datos no son perfectamente separables, se introduce un **t√©rmino de relajaci√≥n** con la funci√≥n de p√©rdida **hinge loss**, lo que lleva a la formulaci√≥n con variables de holgura $\\xi_i$:\n",
    "\n",
    "      $$\n",
    "      \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "      $$\n",
    "\n",
    "      ### Sujeto a:\n",
    "\n",
    "      $$\n",
    "      y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "      $$\n",
    "\n",
    "      Donde \\( C \\) es un hiperpar√°metro que controla la penalizaci√≥n por errores de clasificaci√≥n.\n",
    "   - Responda: ¬øc√≥mo funciona el truco del Kernel para este modelo?  \n",
    "     _(Lo que se espera de esta pregunta es que puedan explicar en sus propias palabras la f√≥rmula a la que llegamos que debemos optimizar de SVM en clase)._\n",
    "\n",
    "     El **truco del Kernel** permite transformar los datos a un espacio de mayor dimensi√≥n donde sean **linealmente separables**, sin necesidad de calcular expl√≠citamente la transformaci√≥n.  \n",
    "      En lugar de calcular la transformaci√≥n $ \\phi(x) $, se usa una funci√≥n **kernel** que eval√∫a el producto escalar en el espacio transformado:\n",
    "\n",
    "      $$\n",
    "      K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "      $$\n",
    "\n",
    "      ### Ejemplos de funciones de kernel comunes:\n",
    "\n",
    "      - **Lineal**:\n",
    "        $$\n",
    "        K(x_i, x_j) = x_i \\cdot x_j\n",
    "        $$\n",
    "\n",
    "      - **Polin√≥mico**:\n",
    "        $$\n",
    "        K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "        $$\n",
    "\n",
    "      - **RBF (Radial Basis Function)**:\n",
    "        $$\n",
    "        K(x_i, x_j) = \\exp(-\\gamma \\| x_i - x_j \\|^2)\n",
    "        $$\n",
    "\n",
    "      Esto permite que **SVM maneje datos no linealmente separables** de manera eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest\n",
    "   - **a.** ¬øQu√© tipo de ensemble learning es este modelo?\n",
    "   \n",
    "   **Random Forest** es un modelo de **Bagging (Bootstrap Aggregating)**, que es un tipo de **ensemble learning basado en agregaci√≥n**.  \n",
    "   - En **Bagging**, se generan m√∫ltiples modelos independientes (en este caso, √°rboles de decisi√≥n) entrenados en diferentes subconjuntos de los datos.  \n",
    "   - Luego, sus predicciones se combinan para mejorar la precisi√≥n y reducir la varianza del modelo.  \n",
    "   - En **clasificaci√≥n**, se usa **votaci√≥n mayoritaria**, y en **regresi√≥n**, se usa el **promedio de las predicciones**.\n",
    "\n",
    "   ---\n",
    "   - **b.** ¬øCu√°l es la idea general detr√°s de Random Forest?\n",
    "\n",
    "\n",
    "   La idea principal de **Random Forest** es entrenar **m√∫ltiples √°rboles de decisi√≥n** en subconjuntos aleatorios de los datos para reducir la varianza y mejorar la precisi√≥n.  \n",
    "El proceso general es el siguiente:\n",
    "\n",
    "1. **Bootstrap Sampling**: Se generan varios subconjuntos de los datos de entrenamiento mediante muestreo con reemplazo.\n",
    "2. **Entrenamiento de √Årboles de Decisi√≥n**: Cada √°rbol se entrena en un subconjunto diferente.\n",
    "3. **Selecci√≥n Aleatoria de Features**: En cada nodo del √°rbol, se selecciona aleatoriamente un subconjunto de variables para dividir los datos.\n",
    "4. **Combinaci√≥n de Predicciones**:\n",
    "   - En **clasificaci√≥n**, se usa **votaci√≥n mayoritaria** entre los √°rboles.\n",
    "   - En **regresi√≥n**, se promedian las predicciones de todos los √°rboles.\n",
    "   Esto hace que **Random Forest sea m√°s estable y generalice mejor** que un solo √°rbol de decisi√≥n, evitando el sobreajuste.\n",
    "\n",
    "---\n",
    "   - **c.** ¬øPor qu√© se busca baja correlaci√≥n entre los √°rboles de Random Forest?\n",
    "\n",
    "\n",
    "   Si los √°rboles dentro del bosque estuvieran altamente correlacionados, **cometer√≠an los mismos errores**, y el ensemble no mejorar√≠a la precisi√≥n.  \n",
    "Por eso, **se busca reducir la correlaci√≥n** entre los √°rboles mediante:\n",
    "- **Bootstrap Sampling**, para que cada √°rbol vea diferentes datos de entrenamiento.\n",
    "- **Selecci√≥n aleatoria de variables en cada nodo**, lo que obliga a los √°rboles a aprender caracter√≠sticas distintas.\n",
    "\n",
    "**Beneficios de una baja correlaci√≥n entre √°rboles:**\n",
    "\n",
    "\n",
    "‚úÖ **Mejora la generalizaci√≥n** ‚Üí Reduce el sobreajuste.  \n",
    "‚úÖ **Aumenta la estabilidad del modelo** ‚Üí Si un √°rbol falla, los otros pueden corregirlo.  \n",
    "‚úÖ **Reduce la varianza** ‚Üí Los errores individuales de los √°rboles se cancelan entre s√≠.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clasificaci√≥n de Mensajes como Spam o Ham usando Bayes/Laplace Smoothing**\n",
    "\n",
    "Este programa entrena un modelo basado en **Naive Bayes** con **Laplace Smoothing** para clasificar mensajes como **ham** (mensaje normal) o **spam** (correo no deseado).\n",
    "\n",
    "## **1. Requisitos del Programa**\n",
    "- Recibir como entrada un archivo llamado **\"entrenamiento.txt\"** (dataset de entrenamiento).\n",
    "- Usar **Naive Bayes** con **Laplace Smoothing** para clasificar mensajes.\n",
    "- Reportar m√©tricas de desempe√±o del modelo.\n",
    "- Clasificar nuevos mensajes como **spam** o **ham**.\n",
    "- **Restricciones:**\n",
    "  - Se permite entrenar **solo un modelo por dataset** (no se pueden cargar m√∫ltiples archivos).\n",
    "  - Se debe **limpiar el dataset** eliminando caracteres especiales y normalizando may√∫sculas/min√∫sculas.\n",
    "  - Cada l√≠nea del dataset representa **un mensaje** con su respectiva categor√≠a.\n",
    "  - Se debe dividir el dataset en **training y test**.\n",
    "  - Se permite usar **librer√≠as externas para la divisi√≥n de datos** (`sklearn`).\n",
    "  - ‚ùå **No se permite usar librer√≠as externas para la construcci√≥n del modelo**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Cargar el Dataset y Preprocesarlo**\n",
    "Primero, **cargamos el dataset** y aplicamos preprocesamiento eliminando caracteres especiales y convirtiendo el texto a min√∫sculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etiqueta</th>\n",
       "      <th>mensaje</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  etiqueta                                            mensaje\n",
       "0      ham  go until jurong point crazy available only in ...\n",
       "1      ham                            ok lar joking wif u oni\n",
       "2     spam  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3      ham        u dun say so early hor u c already then say\n",
       "4      ham  nah i dont think he goes to usf he lives aroun..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def cargar_dataset(nombre_archivo):\n",
    "    datos = []\n",
    "    with open(nombre_archivo, \"r\", encoding=\"utf-8\") as file:\n",
    "        for linea in file:\n",
    "            etiqueta, mensaje = linea.strip().split(\"\\t\")  \n",
    "            mensaje = limpiar_texto(mensaje)  \n",
    "            datos.append((etiqueta, mensaje))\n",
    "    return pd.DataFrame(datos, columns=[\"etiqueta\", \"mensaje\"])\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", texto)  \n",
    "    texto = texto.lower().strip()  \n",
    "    return texto\n",
    "\n",
    "df = cargar_dataset(\"entrenamiento.txt\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Dividir en Training y Test**\n",
    "\n",
    "Se divide el dataset en 80% training y 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del dataset de entrenamiento: 4452\n",
      "Tama√±o del dataset de prueba: 1113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"mensaje\"], df[\"etiqueta\"], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"Tama√±o del dataset de entrenamiento: {len(X_train)}\")\n",
    "print(f\"Tama√±o del dataset de prueba: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Implementar el modelo de Bayes con Laplace Smoothing sin uso de librer√≠as**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesSpamClassifier:\n",
    "    def __init__(self, laplace=1):\n",
    "        # Factor de laplace smoothing\n",
    "        self.laplace = laplace  \n",
    "        self.vocabulario = set()\n",
    "        self.palabras_spam = defaultdict(int)\n",
    "        self.palabras_ham = defaultdict(int)\n",
    "        self.total_spam = 0\n",
    "        self.total_ham = 0\n",
    "        self.num_spam = 0\n",
    "        self.num_ham = 0\n",
    "\n",
    "    def entrenar(self, X, y):\n",
    "        for mensaje, etiqueta in zip(X, y):\n",
    "            palabras = mensaje.split()\n",
    "            self.vocabulario.update(palabras)\n",
    "            if etiqueta == \"spam\":\n",
    "                self.num_spam += 1\n",
    "                for palabra in palabras:\n",
    "                    self.palabras_spam[palabra] += 1\n",
    "                    self.total_spam += 1\n",
    "            else:\n",
    "                self.num_ham += 1\n",
    "                for palabra in palabras:\n",
    "                    self.palabras_ham[palabra] += 1\n",
    "                    self.total_ham += 1\n",
    "\n",
    "    def predecir(self, mensaje):\n",
    "        palabras = mensaje.split()\n",
    "        vocab_size = len(self.vocabulario)\n",
    "\n",
    "        # Probabilidades iniciales\n",
    "        p_spam = np.log(self.num_spam / (self.num_spam + self.num_ham))\n",
    "        p_ham = np.log(self.num_ham / (self.num_spam + self.num_ham))\n",
    "\n",
    "        # Calcular probabilidades con Laplace Smoothing\n",
    "        for palabra in palabras:\n",
    "            p_spam += np.log((self.palabras_spam[palabra] + self.laplace) / (self.total_spam + vocab_size * self.laplace))\n",
    "            p_ham += np.log((self.palabras_ham[palabra] + self.laplace) / (self.total_ham + vocab_size * self.laplace))\n",
    "\n",
    "        return \"spam\" if p_spam > p_ham else \"ham\"\n",
    "\n",
    "\n",
    "modelo = NaiveBayesSpamClassifier()\n",
    "modelo.entrenar(X_train, y_train)\n",
    "print(\"Modelo entrenado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Evaluar Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar el modelo usamos:\n",
    "- **Precisi√≥n (Precision)**: Mide cu√°ntos de los mensajes clasificados como spam realmente lo son.\n",
    "- **Recall (Sensibilidad)**: Mide cu√°ntos de los mensajes spam fueron detectados correctamente.\n",
    "- **F1-score**: Es el balance entre precisi√≥n y recall.\n",
    "\n",
    "Estas m√©tricas son importantes en clasificaci√≥n de spam, ya que un **bajo recall** significa que el modelo no detecta suficiente spam, y un **bajo precision** significa que clasifica err√≥neamente mensajes ham como spam.ara evaluar el rendimiento del modelo, calculamos precisi√≥n, recall y f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m [\u001b[43mmodelo\u001b[49m\u001b[38;5;241m.\u001b[39mpredecir(mensaje) \u001b[38;5;28;01mfor\u001b[39;00m mensaje \u001b[38;5;129;01min\u001b[39;00m X_test]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mham\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspam\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modelo' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_pred = [modelo.predecir(mensaje) for mensaje in X_test]\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"ham\", \"spam\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Predicci√≥n Mensajes Futuros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì© Ingrese mensajes para clasificar. Escriba 'salir' para terminar.\n",
      "\n",
      "üîç **Resultados de clasificaci√≥n:**\n",
      "üì© Mensaje ingresado: vamos a la oficina\n",
      "üìå Probabilidad de Spam: 0.1233\n",
      "üìå Probabilidad de Ham: 0.8767\n",
      "‚úÖ **El mensaje ha sido clasificado como: HAM**\n",
      "‚ö†Ô∏è Palabras no vistas en el entrenamiento: vamos, oficina\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "üîç **Resultados de clasificaci√≥n:**\n",
      "üì© Mensaje ingresado: ganate un premio facil!\n",
      "üìå Probabilidad de Spam: 0.7217\n",
      "üìå Probabilidad de Ham: 0.2783\n",
      "‚úÖ **El mensaje ha sido clasificado como: SPAM**\n",
      "‚ö†Ô∏è Palabras no vistas en el entrenamiento: ganate, premio, facil\n",
      "\n",
      "==================================================\n",
      "\n",
      "üö™ Saliendo del programa...\n"
     ]
    }
   ],
   "source": [
    "def predecir_probabilidades(modelo, mensaje):\n",
    "    mensaje = limpiar_texto(mensaje)  \n",
    "    palabras = mensaje.split()\n",
    "    vocab_size = len(modelo.vocabulario)\n",
    "\n",
    "    p_spam_log = np.log(modelo.num_spam / (modelo.num_spam + modelo.num_ham))\n",
    "    p_ham_log = np.log(modelo.num_ham / (modelo.num_spam + modelo.num_ham))\n",
    "\n",
    "    palabras_no_vistas = [palabra for palabra in palabras if palabra not in modelo.vocabulario]\n",
    "\n",
    "    for palabra in palabras:\n",
    "        p_spam_log += np.log((modelo.palabras_spam[palabra] + modelo.laplace) / (modelo.total_spam + vocab_size * modelo.laplace))\n",
    "        p_ham_log += np.log((modelo.palabras_ham[palabra] + modelo.laplace) / (modelo.total_ham + vocab_size * modelo.laplace))\n",
    "\n",
    "    p_spam = np.exp(p_spam_log)\n",
    "    p_ham = np.exp(p_ham_log)\n",
    "\n",
    "    total = p_spam + p_ham\n",
    "    p_spam /= total\n",
    "    p_ham /= total\n",
    "\n",
    "    clasificacion = \"spam\" if p_spam > p_ham else \"ham\"\n",
    "\n",
    "    return p_spam, p_ham, clasificacion, palabras_no_vistas\n",
    "\n",
    "modelo = NaiveBayesSpamClassifier(laplace=0.1)\n",
    "modelo.entrenar(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"üì© Ingrese mensajes para clasificar. Escriba 'salir' para terminar.\")\n",
    "\n",
    "while True:\n",
    "    mensaje = input(\"‚úèÔ∏è  Escriba un mensaje: \").strip()\n",
    "    if mensaje.lower() == \"salir\":\n",
    "        print(\"üö™ Saliendo del programa...\")\n",
    "        break\n",
    "\n",
    "    p_spam, p_ham, clasificacion, palabras_no_vistas = predecir_probabilidades(modelo, mensaje)\n",
    "\n",
    "    print(f\"\\nüîç **Resultados de clasificaci√≥n:**\")\n",
    "    print(f\"üì© Mensaje ingresado: {mensaje}\")\n",
    "    print(f\"üìå Probabilidad de Spam: {p_spam:.4f}\")\n",
    "    print(f\"üìå Probabilidad de Ham: {p_ham:.4f}\")\n",
    "    print(f\"‚úÖ **El mensaje ha sido clasificado como: {clasificacion.upper()}**\")\n",
    "    \n",
    "    if palabras_no_vistas:\n",
    "        print(f\"‚ö†Ô∏è Palabras no vistas en el entrenamiento: {', '.join(palabras_no_vistas)}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Evaluaci√≥n Modelo con librer√≠as**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del dataset de entrenamiento: 4452\n",
      "Tama√±o del dataset de prueba: 1113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower().strip()\n",
    "    texto = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", texto)  \n",
    "    return texto\n",
    "\n",
    "def cargar_dataset(nombre_archivo):\n",
    "    datos = []\n",
    "    with open(nombre_archivo, \"r\", encoding=\"utf-8\") as file:\n",
    "        for linea in file:\n",
    "            etiqueta, mensaje = linea.strip().split(\"\\t\")  \n",
    "            mensaje = limpiar_texto(mensaje)  \n",
    "            datos.append((etiqueta, mensaje))\n",
    "    return pd.DataFrame(datos, columns=[\"etiqueta\", \"mensaje\"])\n",
    "\n",
    "df = cargar_dataset(\"entrenamiento.txt\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"mensaje\"], df[\"etiqueta\"], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Tama√±o del dataset de entrenamiento: {len(X_train)}\")\n",
    "print(f\"Tama√±o del dataset de prueba: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justificaci√≥n de las m√©tricas utilizadas**\n",
    "- **Precisi√≥n (Precision)**: Mide cu√°ntos de los mensajes clasificados como spam realmente lo son.\n",
    "  ‚Üí Es importante en este problema porque un **falso positivo** (clasificar un mensaje ham como spam) es cr√≠tico.\n",
    "  \n",
    "- **Recall (Sensibilidad)**: Mide cu√°ntos de los mensajes spam fueron detectados correctamente.\n",
    "  ‚Üí Un **bajo recall** significa que el modelo no detecta suficiente spam.\n",
    "\n",
    "- **F1-score**: Es el balance entre precisi√≥n y recall.\n",
    "  ‚Üí Se usa en problemas desbalanceados como clasificaci√≥n de spam, donde queremos reducir **falsos positivos y falsos negativos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä **Evaluaci√≥n del modelo con `MultinomialNB` de sklearn:**\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.97       950\n",
      "        spam       1.00      0.67      0.80       163\n",
      "\n",
      "    accuracy                           0.95      1113\n",
      "   macro avg       0.97      0.83      0.89      1113\n",
      "weighted avg       0.95      0.95      0.95      1113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "modelo_sklearn = make_pipeline(vectorizer, MultinomialNB())\n",
    "modelo_sklearn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sklearn = modelo_sklearn.predict(X_test)\n",
    "\n",
    "print(\"üìä **Evaluaci√≥n del modelo con `MultinomialNB` de sklearn:**\")\n",
    "print(classification_report(y_test, y_pred_sklearn, target_names=[\"ham\", \"spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Clasificaci√≥n de Partidas de League of Legends**\n",
    "\n",
    "## **Introducci√≥n**\n",
    "League of Legends es un juego online multijugador categorizado como MOBA (Multiplayer Online Battle Arena). En este juego, dos equipos (azul y rojo) de cinco jugadores compiten con el objetivo de derribar el Nexus enemigo.\n",
    "\n",
    "El prop√≥sito de este proyecto es construir un modelo de clasificaci√≥n que prediga si el equipo azul gana una partida, utilizando un conjunto de datos de League of Legends. \n",
    "\n",
    "## **Exploraci√≥n de Datos**\n",
    "En esta secci√≥n realizaremos una exploraci√≥n inicial del dataset para comprender su estructura y prepararlo para el modelo de clasificaci√≥n.\n",
    "\n",
    "### **1. Carga de Datos**\n",
    "- Cargar el dataset en un DataFrame de Pandas.\n",
    "- Inspeccionar las primeras filas y verificar informaci√≥n general de las variables.\n",
    "\n",
    "### **2. Preprocesamiento de Datos**\n",
    "- **Codificaci√≥n de variables**: Transformar variables categ√≥ricas en num√©ricas si es necesario.\n",
    "- **Balanceo del dataset**: Verificar si la variable objetivo est√° balanceada y, si no lo est√°, aplicar t√©cnicas de balanceo.\n",
    "- **Escalado de variables**: Normalizar o estandarizar las caracter√≠sticas si es necesario.\n",
    "- **Selecci√≥n de caracter√≠sticas**: Determinar qu√© variables son m√°s relevantes para el modelo.\n",
    "\n",
    "## **Divisi√≥n del Dataset**\n",
    "El dataset se dividir√° en:\n",
    "- **80%** para entrenamiento.\n",
    "- **20%** para prueba.\n",
    "- **Opcionalmente**, si se requiere, el conjunto de prueba se subdividir√° en **10%** para validaci√≥n y **90%** para evaluaci√≥n.\n",
    "\n",
    "## **Construcci√≥n del Modelo**\n",
    "- Selecci√≥n del modelo de clasificaci√≥n adecuado.\n",
    "- Entrenamiento del modelo con los datos preparados.\n",
    "- Evaluaci√≥n del desempe√±o del modelo con m√©tricas adecuadas.\n",
    "\n",
    "## **M√©trica de Desempe√±o**\n",
    "Para evaluar el modelo, se seleccionar√° una m√©trica de desempe√±o basada en la naturaleza del problema:\n",
    "- **Exactitud (Accuracy)**: √ötil si el dataset est√° balanceado.\n",
    "- **F1-Score**: Adecuado en caso de un dataset desbalanceado.\n",
    "- **ROC-AUC**: Para evaluar la capacidad del modelo de distinguir entre clases positivas y negativas.\n",
    "\n",
    "Se justificar√° la elecci√≥n de la m√©trica en funci√≥n del an√°lisis del dataset.\n",
    "\n",
    "## **Conclusi√≥n**\n",
    "Se analizar√°n los resultados obtenidos y se discutir√°n mejoras posibles en la clasificaci√≥n de partidas de League of Legends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sjsj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = \"high_diamond_ranked_10min.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Seleccionar caracter√≠sticas y la variable objetivo\n",
    "target = \"blueWins\"\n",
    "features = [\"blueGoldDiff\", \"blueExperienceDiff\"]\n",
    "data = data[features + [target]]\n",
    "\n",
    "# Normalizaci√≥n de datos\n",
    "data[features] = (data[features] - data[features].mean()) / data[features].std()\n",
    "\n",
    "# Divisi√≥n de datos\n",
    "X = data[features].values\n",
    "y = data[target].values\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
