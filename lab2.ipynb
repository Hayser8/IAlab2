{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Laboratorio 2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Julio García Salas - 22076**\n",
    "#### **Sofía García -22210**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Por qué el modelo de Naive Bayes se le considera “naive”?\n",
    "\n",
    "El modelo de Naive Bayes se considera \"naive\" (ingenuo) porque asume que todas las variables predictoras (features) son independientes entre sí, lo cual rara vez es cierto en la práctica. Esta suposición simplifica enormemente los cálculos, permitiendo una clasificación rápida y eficiente, aunque en muchos casos la independencia condicional no se cumple completamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machine (SVM)\n",
    "   - Explique la formulación matemática que se busca optimizar en Support Vector Machine.\n",
    "      El objetivo de **SVM** es encontrar un hiperplano que maximice la **margen** entre dos clases de datos.  \n",
    "      La formulación matemática de SVM se basa en minimizar la siguiente función objetivo:\n",
    "\n",
    "      $$\n",
    "      \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "      $$\n",
    "\n",
    "      ### Sujeto a las restricciones:\n",
    "\n",
    "      $$\n",
    "      y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "      $$\n",
    "\n",
    "      ### Donde:\n",
    "\n",
    "      **Vector de pesos:**  \n",
    "      $$\n",
    "      \\mathbf{w}\n",
    "      $$\n",
    "\n",
    "      **Sesgo (*bias*):**  \n",
    "      $$\n",
    "      b\n",
    "      $$\n",
    "\n",
    "      **Cada punto de datos de entrenamiento:**  \n",
    "      $$\n",
    "      \\mathbf{x}_i\n",
    "      $$\n",
    "\n",
    "      **Etiqueta de la clase:**  \n",
    "      $$\n",
    "      y_i \\in \\{+1, -1\\}\n",
    "      $$\n",
    "      \n",
    "\n",
    "      Si los datos no son perfectamente separables, se introduce un **término de relajación** con la función de pérdida **hinge loss**, lo que lleva a la formulación con variables de holgura $\\xi_i$:\n",
    "\n",
    "      $$\n",
    "      \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "      $$\n",
    "\n",
    "      ### Sujeto a:\n",
    "\n",
    "      $$\n",
    "      y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "      $$\n",
    "\n",
    "      Donde \\( C \\) es un hiperparámetro que controla la penalización por errores de clasificación.\n",
    "   - Responda: ¿cómo funciona el truco del Kernel para este modelo?  \n",
    "     _(Lo que se espera de esta pregunta es que puedan explicar en sus propias palabras la fórmula a la que llegamos que debemos optimizar de SVM en clase)._\n",
    "\n",
    "     El **truco del Kernel** permite transformar los datos a un espacio de mayor dimensión donde sean **linealmente separables**, sin necesidad de calcular explícitamente la transformación.  \n",
    "      En lugar de calcular la transformación $ \\phi(x) $, se usa una función **kernel** que evalúa el producto escalar en el espacio transformado:\n",
    "\n",
    "      $$\n",
    "      K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "      $$\n",
    "\n",
    "      ### Ejemplos de funciones de kernel comunes:\n",
    "\n",
    "      - **Lineal**:\n",
    "        $$\n",
    "        K(x_i, x_j) = x_i \\cdot x_j\n",
    "        $$\n",
    "\n",
    "      - **Polinómico**:\n",
    "        $$\n",
    "        K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "        $$\n",
    "\n",
    "      - **RBF (Radial Basis Function)**:\n",
    "        $$\n",
    "        K(x_i, x_j) = \\exp(-\\gamma \\| x_i - x_j \\|^2)\n",
    "        $$\n",
    "\n",
    "      Esto permite que **SVM maneje datos no linealmente separables** de manera eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest\n",
    "   - **a.** ¿Qué tipo de ensemble learning es este modelo?\n",
    "   \n",
    "   **Random Forest** es un modelo de **Bagging (Bootstrap Aggregating)**, que es un tipo de **ensemble learning basado en agregación**.  \n",
    "   - En **Bagging**, se generan múltiples modelos independientes (en este caso, árboles de decisión) entrenados en diferentes subconjuntos de los datos.  \n",
    "   - Luego, sus predicciones se combinan para mejorar la precisión y reducir la varianza del modelo.  \n",
    "   - En **clasificación**, se usa **votación mayoritaria**, y en **regresión**, se usa el **promedio de las predicciones**.\n",
    "\n",
    "   ---\n",
    "   - **b.** ¿Cuál es la idea general detrás de Random Forest?\n",
    "\n",
    "\n",
    "   La idea principal de **Random Forest** es entrenar **múltiples árboles de decisión** en subconjuntos aleatorios de los datos para reducir la varianza y mejorar la precisión.  \n",
    "El proceso general es el siguiente:\n",
    "\n",
    "1. **Bootstrap Sampling**: Se generan varios subconjuntos de los datos de entrenamiento mediante muestreo con reemplazo.\n",
    "2. **Entrenamiento de Árboles de Decisión**: Cada árbol se entrena en un subconjunto diferente.\n",
    "3. **Selección Aleatoria de Features**: En cada nodo del árbol, se selecciona aleatoriamente un subconjunto de variables para dividir los datos.\n",
    "4. **Combinación de Predicciones**:\n",
    "   - En **clasificación**, se usa **votación mayoritaria** entre los árboles.\n",
    "   - En **regresión**, se promedian las predicciones de todos los árboles.\n",
    "   Esto hace que **Random Forest sea más estable y generalice mejor** que un solo árbol de decisión, evitando el sobreajuste.\n",
    "\n",
    "---\n",
    "   - **c.** ¿Por qué se busca baja correlación entre los árboles de Random Forest?\n",
    "\n",
    "\n",
    "   Si los árboles dentro del bosque estuvieran altamente correlacionados, **cometerían los mismos errores**, y el ensemble no mejoraría la precisión.  \n",
    "Por eso, **se busca reducir la correlación** entre los árboles mediante:\n",
    "- **Bootstrap Sampling**, para que cada árbol vea diferentes datos de entrenamiento.\n",
    "- **Selección aleatoria de variables en cada nodo**, lo que obliga a los árboles a aprender características distintas.\n",
    "\n",
    "**Beneficios de una baja correlación entre árboles:**\n",
    "\n",
    "\n",
    "✅ **Mejora la generalización** → Reduce el sobreajuste.  \n",
    "✅ **Aumenta la estabilidad del modelo** → Si un árbol falla, los otros pueden corregirlo.  \n",
    "✅ **Reduce la varianza** → Los errores individuales de los árboles se cancelan entre sí.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
