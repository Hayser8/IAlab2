{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Laboratorio 2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Julio García Salas - 22076**\n",
    "#### **Sofía García -22210**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Por qué el modelo de Naive Bayes se le considera “naive”?\n",
    "\n",
    "El modelo de Naive Bayes se considera \"naive\" (ingenuo) porque asume que todas las variables predictoras (features) son independientes entre sí, lo cual rara vez es cierto en la práctica. Esta suposición simplifica enormemente los cálculos, permitiendo una clasificación rápida y eficiente, aunque en muchos casos la independencia condicional no se cumple completamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machine (SVM)\n",
    "   - Explique la formulación matemática que se busca optimizar en Support Vector Machine.\n",
    "      El objetivo de **SVM** es encontrar un hiperplano que maximice la **margen** entre dos clases de datos.  \n",
    "      La formulación matemática de SVM se basa en minimizar la siguiente función objetivo:\n",
    "\n",
    "      $$\n",
    "      \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "      $$\n",
    "\n",
    "      ### Sujeto a las restricciones:\n",
    "\n",
    "      $$\n",
    "      y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "      $$\n",
    "\n",
    "      ### Donde:\n",
    "\n",
    "      **Vector de pesos:**  \n",
    "      $$\n",
    "      \\mathbf{w}\n",
    "      $$\n",
    "\n",
    "      **Sesgo (*bias*):**  \n",
    "      $$\n",
    "      b\n",
    "      $$\n",
    "\n",
    "      **Cada punto de datos de entrenamiento:**  \n",
    "      $$\n",
    "      \\mathbf{x}_i\n",
    "      $$\n",
    "\n",
    "      **Etiqueta de la clase:**  \n",
    "      $$\n",
    "      y_i \\in \\{+1, -1\\}\n",
    "      $$\n",
    "      \n",
    "\n",
    "      Si los datos no son perfectamente separables, se introduce un **término de relajación** con la función de pérdida **hinge loss**, lo que lleva a la formulación con variables de holgura $\\xi_i$:\n",
    "\n",
    "      $$\n",
    "      \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "      $$\n",
    "\n",
    "      ### Sujeto a:\n",
    "\n",
    "      $$\n",
    "      y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "      $$\n",
    "\n",
    "      Donde \\( C \\) es un hiperparámetro que controla la penalización por errores de clasificación.\n",
    "   - Responda: ¿cómo funciona el truco del Kernel para este modelo?  \n",
    "     _(Lo que se espera de esta pregunta es que puedan explicar en sus propias palabras la fórmula a la que llegamos que debemos optimizar de SVM en clase)._\n",
    "\n",
    "     El **truco del Kernel** permite transformar los datos a un espacio de mayor dimensión donde sean **linealmente separables**, sin necesidad de calcular explícitamente la transformación.  \n",
    "      En lugar de calcular la transformación $ \\phi(x) $, se usa una función **kernel** que evalúa el producto escalar en el espacio transformado:\n",
    "\n",
    "      $$\n",
    "      K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "      $$\n",
    "\n",
    "      ### Ejemplos de funciones de kernel comunes:\n",
    "\n",
    "      - **Lineal**:\n",
    "        $$\n",
    "        K(x_i, x_j) = x_i \\cdot x_j\n",
    "        $$\n",
    "\n",
    "      - **Polinómico**:\n",
    "        $$\n",
    "        K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "        $$\n",
    "\n",
    "      - **RBF (Radial Basis Function)**:\n",
    "        $$\n",
    "        K(x_i, x_j) = \\exp(-\\gamma \\| x_i - x_j \\|^2)\n",
    "        $$\n",
    "\n",
    "      Esto permite que **SVM maneje datos no linealmente separables** de manera eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest\n",
    "   - **a.** ¿Qué tipo de ensemble learning es este modelo?\n",
    "   \n",
    "   **Random Forest** es un modelo de **Bagging (Bootstrap Aggregating)**, que es un tipo de **ensemble learning basado en agregación**.  \n",
    "   - En **Bagging**, se generan múltiples modelos independientes (en este caso, árboles de decisión) entrenados en diferentes subconjuntos de los datos.  \n",
    "   - Luego, sus predicciones se combinan para mejorar la precisión y reducir la varianza del modelo.  \n",
    "   - En **clasificación**, se usa **votación mayoritaria**, y en **regresión**, se usa el **promedio de las predicciones**.\n",
    "\n",
    "   ---\n",
    "   - **b.** ¿Cuál es la idea general detrás de Random Forest?\n",
    "\n",
    "\n",
    "   La idea principal de **Random Forest** es entrenar **múltiples árboles de decisión** en subconjuntos aleatorios de los datos para reducir la varianza y mejorar la precisión.  \n",
    "El proceso general es el siguiente:\n",
    "\n",
    "1. **Bootstrap Sampling**: Se generan varios subconjuntos de los datos de entrenamiento mediante muestreo con reemplazo.\n",
    "2. **Entrenamiento de Árboles de Decisión**: Cada árbol se entrena en un subconjunto diferente.\n",
    "3. **Selección Aleatoria de Features**: En cada nodo del árbol, se selecciona aleatoriamente un subconjunto de variables para dividir los datos.\n",
    "4. **Combinación de Predicciones**:\n",
    "   - En **clasificación**, se usa **votación mayoritaria** entre los árboles.\n",
    "   - En **regresión**, se promedian las predicciones de todos los árboles.\n",
    "   Esto hace que **Random Forest sea más estable y generalice mejor** que un solo árbol de decisión, evitando el sobreajuste.\n",
    "\n",
    "---\n",
    "   - **c.** ¿Por qué se busca baja correlación entre los árboles de Random Forest?\n",
    "\n",
    "\n",
    "   Si los árboles dentro del bosque estuvieran altamente correlacionados, **cometerían los mismos errores**, y el ensemble no mejoraría la precisión.  \n",
    "Por eso, **se busca reducir la correlación** entre los árboles mediante:\n",
    "- **Bootstrap Sampling**, para que cada árbol vea diferentes datos de entrenamiento.\n",
    "- **Selección aleatoria de variables en cada nodo**, lo que obliga a los árboles a aprender características distintas.\n",
    "\n",
    "**Beneficios de una baja correlación entre árboles:**\n",
    "\n",
    "\n",
    "✅ **Mejora la generalización** → Reduce el sobreajuste.  \n",
    "✅ **Aumenta la estabilidad del modelo** → Si un árbol falla, los otros pueden corregirlo.  \n",
    "✅ **Reduce la varianza** → Los errores individuales de los árboles se cancelan entre sí.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clasificación de Mensajes como Spam o Ham usando Bayes/Laplace Smoothing**\n",
    "\n",
    "Este programa entrena un modelo basado en **Naive Bayes** con **Laplace Smoothing** para clasificar mensajes como **ham** (mensaje normal) o **spam** (correo no deseado).\n",
    "\n",
    "## **1. Requisitos del Programa**\n",
    "- Recibir como entrada un archivo llamado **\"entrenamiento.txt\"** (dataset de entrenamiento).\n",
    "- Usar **Naive Bayes** con **Laplace Smoothing** para clasificar mensajes.\n",
    "- Reportar métricas de desempeño del modelo.\n",
    "- Clasificar nuevos mensajes como **spam** o **ham**.\n",
    "- **Restricciones:**\n",
    "  - Se permite entrenar **solo un modelo por dataset** (no se pueden cargar múltiples archivos).\n",
    "  - Se debe **limpiar el dataset** eliminando caracteres especiales y normalizando mayúsculas/minúsculas.\n",
    "  - Cada línea del dataset representa **un mensaje** con su respectiva categoría.\n",
    "  - Se debe dividir el dataset en **training y test**.\n",
    "  - Se permite usar **librerías externas para la división de datos** (`sklearn`).\n",
    "  - ❌ **No se permite usar librerías externas para la construcción del modelo**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Cargar el Dataset y Preprocesarlo**\n",
    "Primero, **cargamos el dataset** y aplicamos preprocesamiento eliminando caracteres especiales y convirtiendo el texto a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etiqueta</th>\n",
       "      <th>mensaje</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  etiqueta                                            mensaje\n",
       "0      ham  go until jurong point crazy available only in ...\n",
       "1      ham                            ok lar joking wif u oni\n",
       "2     spam  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3      ham        u dun say so early hor u c already then say\n",
       "4      ham  nah i dont think he goes to usf he lives aroun..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def cargar_dataset(nombre_archivo):\n",
    "    datos = []\n",
    "    with open(nombre_archivo, \"r\", encoding=\"utf-8\") as file:\n",
    "        for linea in file:\n",
    "            etiqueta, mensaje = linea.strip().split(\"\\t\")  \n",
    "            mensaje = limpiar_texto(mensaje)  \n",
    "            datos.append((etiqueta, mensaje))\n",
    "    return pd.DataFrame(datos, columns=[\"etiqueta\", \"mensaje\"])\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", texto)  \n",
    "    texto = texto.lower().strip()  \n",
    "    return texto\n",
    "\n",
    "df = cargar_dataset(\"entrenamiento.txt\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Dividir en Training y Test**\n",
    "\n",
    "Se divide el dataset en 80% training y 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de entrenamiento: 4452\n",
      "Tamaño del dataset de prueba: 1113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"mensaje\"], df[\"etiqueta\"], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"Tamaño del dataset de entrenamiento: {len(X_train)}\")\n",
    "print(f\"Tamaño del dataset de prueba: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Implementar el modelo de Bayes con Laplace Smoothing sin uso de librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesSpamClassifier:\n",
    "    def __init__(self, laplace=1):\n",
    "        # Factor de laplace smoothing\n",
    "        self.laplace = laplace  \n",
    "        self.vocabulario = set()\n",
    "        self.palabras_spam = defaultdict(int)\n",
    "        self.palabras_ham = defaultdict(int)\n",
    "        self.total_spam = 0\n",
    "        self.total_ham = 0\n",
    "        self.num_spam = 0\n",
    "        self.num_ham = 0\n",
    "\n",
    "    def entrenar(self, X, y):\n",
    "        for mensaje, etiqueta in zip(X, y):\n",
    "            palabras = mensaje.split()\n",
    "            self.vocabulario.update(palabras)\n",
    "            if etiqueta == \"spam\":\n",
    "                self.num_spam += 1\n",
    "                for palabra in palabras:\n",
    "                    self.palabras_spam[palabra] += 1\n",
    "                    self.total_spam += 1\n",
    "            else:\n",
    "                self.num_ham += 1\n",
    "                for palabra in palabras:\n",
    "                    self.palabras_ham[palabra] += 1\n",
    "                    self.total_ham += 1\n",
    "\n",
    "    def predecir(self, mensaje):\n",
    "        palabras = mensaje.split()\n",
    "        vocab_size = len(self.vocabulario)\n",
    "\n",
    "        # Probabilidades iniciales\n",
    "        p_spam = np.log(self.num_spam / (self.num_spam + self.num_ham))\n",
    "        p_ham = np.log(self.num_ham / (self.num_spam + self.num_ham))\n",
    "\n",
    "        # Calcular probabilidades con Laplace Smoothing\n",
    "        for palabra in palabras:\n",
    "            p_spam += np.log((self.palabras_spam[palabra] + self.laplace) / (self.total_spam + vocab_size * self.laplace))\n",
    "            p_ham += np.log((self.palabras_ham[palabra] + self.laplace) / (self.total_ham + vocab_size * self.laplace))\n",
    "\n",
    "        return \"spam\" if p_spam > p_ham else \"ham\"\n",
    "\n",
    "\n",
    "modelo = NaiveBayesSpamClassifier()\n",
    "modelo.entrenar(X_train, y_train)\n",
    "print(\"Modelo entrenado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Evaluar Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar el modelo usamos:\n",
    "- **Precisión (Precision)**: Mide cuántos de los mensajes clasificados como spam realmente lo son.\n",
    "- **Recall (Sensibilidad)**: Mide cuántos de los mensajes spam fueron detectados correctamente.\n",
    "- **F1-score**: Es el balance entre precisión y recall.\n",
    "\n",
    "Estas métricas son importantes en clasificación de spam, ya que un **bajo recall** significa que el modelo no detecta suficiente spam, y un **bajo precision** significa que clasifica erróneamente mensajes ham como spam.ara evaluar el rendimiento del modelo, calculamos precisión, recall y f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m [\u001b[43mmodelo\u001b[49m\u001b[38;5;241m.\u001b[39mpredecir(mensaje) \u001b[38;5;28;01mfor\u001b[39;00m mensaje \u001b[38;5;129;01min\u001b[39;00m X_test]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mham\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspam\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modelo' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_pred = [modelo.predecir(mensaje) for mensaje in X_test]\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"ham\", \"spam\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Predicción Mensajes Futuros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📩 Ingrese mensajes para clasificar. Escriba 'salir' para terminar.\n",
      "\n",
      "🔍 **Resultados de clasificación:**\n",
      "📩 Mensaje ingresado: vamos a la oficina\n",
      "📌 Probabilidad de Spam: 0.1233\n",
      "📌 Probabilidad de Ham: 0.8767\n",
      "✅ **El mensaje ha sido clasificado como: HAM**\n",
      "⚠️ Palabras no vistas en el entrenamiento: vamos, oficina\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "🔍 **Resultados de clasificación:**\n",
      "📩 Mensaje ingresado: ganate un premio facil!\n",
      "📌 Probabilidad de Spam: 0.7217\n",
      "📌 Probabilidad de Ham: 0.2783\n",
      "✅ **El mensaje ha sido clasificado como: SPAM**\n",
      "⚠️ Palabras no vistas en el entrenamiento: ganate, premio, facil\n",
      "\n",
      "==================================================\n",
      "\n",
      "🚪 Saliendo del programa...\n"
     ]
    }
   ],
   "source": [
    "def predecir_probabilidades(modelo, mensaje):\n",
    "    mensaje = limpiar_texto(mensaje)  \n",
    "    palabras = mensaje.split()\n",
    "    vocab_size = len(modelo.vocabulario)\n",
    "\n",
    "    p_spam_log = np.log(modelo.num_spam / (modelo.num_spam + modelo.num_ham))\n",
    "    p_ham_log = np.log(modelo.num_ham / (modelo.num_spam + modelo.num_ham))\n",
    "\n",
    "    palabras_no_vistas = [palabra for palabra in palabras if palabra not in modelo.vocabulario]\n",
    "\n",
    "    for palabra in palabras:\n",
    "        p_spam_log += np.log((modelo.palabras_spam[palabra] + modelo.laplace) / (modelo.total_spam + vocab_size * modelo.laplace))\n",
    "        p_ham_log += np.log((modelo.palabras_ham[palabra] + modelo.laplace) / (modelo.total_ham + vocab_size * modelo.laplace))\n",
    "\n",
    "    p_spam = np.exp(p_spam_log)\n",
    "    p_ham = np.exp(p_ham_log)\n",
    "\n",
    "    total = p_spam + p_ham\n",
    "    p_spam /= total\n",
    "    p_ham /= total\n",
    "\n",
    "    clasificacion = \"spam\" if p_spam > p_ham else \"ham\"\n",
    "\n",
    "    return p_spam, p_ham, clasificacion, palabras_no_vistas\n",
    "\n",
    "modelo = NaiveBayesSpamClassifier(laplace=0.1)\n",
    "modelo.entrenar(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"📩 Ingrese mensajes para clasificar. Escriba 'salir' para terminar.\")\n",
    "\n",
    "while True:\n",
    "    mensaje = input(\"✏️  Escriba un mensaje: \").strip()\n",
    "    if mensaje.lower() == \"salir\":\n",
    "        print(\"🚪 Saliendo del programa...\")\n",
    "        break\n",
    "\n",
    "    p_spam, p_ham, clasificacion, palabras_no_vistas = predecir_probabilidades(modelo, mensaje)\n",
    "\n",
    "    print(f\"\\n🔍 **Resultados de clasificación:**\")\n",
    "    print(f\"📩 Mensaje ingresado: {mensaje}\")\n",
    "    print(f\"📌 Probabilidad de Spam: {p_spam:.4f}\")\n",
    "    print(f\"📌 Probabilidad de Ham: {p_ham:.4f}\")\n",
    "    print(f\"✅ **El mensaje ha sido clasificado como: {clasificacion.upper()}**\")\n",
    "    \n",
    "    if palabras_no_vistas:\n",
    "        print(f\"⚠️ Palabras no vistas en el entrenamiento: {', '.join(palabras_no_vistas)}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Evaluación Modelo con librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de entrenamiento: 4452\n",
      "Tamaño del dataset de prueba: 1113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower().strip()\n",
    "    texto = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", texto)  \n",
    "    return texto\n",
    "\n",
    "def cargar_dataset(nombre_archivo):\n",
    "    datos = []\n",
    "    with open(nombre_archivo, \"r\", encoding=\"utf-8\") as file:\n",
    "        for linea in file:\n",
    "            etiqueta, mensaje = linea.strip().split(\"\\t\")  \n",
    "            mensaje = limpiar_texto(mensaje)  \n",
    "            datos.append((etiqueta, mensaje))\n",
    "    return pd.DataFrame(datos, columns=[\"etiqueta\", \"mensaje\"])\n",
    "\n",
    "df = cargar_dataset(\"entrenamiento.txt\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"mensaje\"], df[\"etiqueta\"], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Tamaño del dataset de entrenamiento: {len(X_train)}\")\n",
    "print(f\"Tamaño del dataset de prueba: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justificación de las métricas utilizadas**\n",
    "- **Precisión (Precision)**: Mide cuántos de los mensajes clasificados como spam realmente lo son.\n",
    "  → Es importante en este problema porque un **falso positivo** (clasificar un mensaje ham como spam) es crítico.\n",
    "  \n",
    "- **Recall (Sensibilidad)**: Mide cuántos de los mensajes spam fueron detectados correctamente.\n",
    "  → Un **bajo recall** significa que el modelo no detecta suficiente spam.\n",
    "\n",
    "- **F1-score**: Es el balance entre precisión y recall.\n",
    "  → Se usa en problemas desbalanceados como clasificación de spam, donde queremos reducir **falsos positivos y falsos negativos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 **Evaluación del modelo con `MultinomialNB` de sklearn:**\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.97       950\n",
      "        spam       1.00      0.67      0.80       163\n",
      "\n",
      "    accuracy                           0.95      1113\n",
      "   macro avg       0.97      0.83      0.89      1113\n",
      "weighted avg       0.95      0.95      0.95      1113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "modelo_sklearn = make_pipeline(vectorizer, MultinomialNB())\n",
    "modelo_sklearn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sklearn = modelo_sklearn.predict(X_test)\n",
    "\n",
    "print(\"📊 **Evaluación del modelo con `MultinomialNB` de sklearn:**\")\n",
    "print(classification_report(y_test, y_pred_sklearn, target_names=[\"ham\", \"spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Clasificación de Partidas de League of Legends**\n",
    "\n",
    "## **Introducción**\n",
    "League of Legends es un juego online multijugador categorizado como MOBA (Multiplayer Online Battle Arena). En este juego, dos equipos (azul y rojo) de cinco jugadores compiten con el objetivo de derribar el Nexus enemigo.\n",
    "\n",
    "El propósito de este proyecto es construir un modelo de clasificación que prediga si el equipo azul gana una partida, utilizando un conjunto de datos de League of Legends. \n",
    "\n",
    "## **Exploración de Datos**\n",
    "En esta sección realizaremos una exploración inicial del dataset para comprender su estructura y prepararlo para el modelo de clasificación.\n",
    "\n",
    "### **1. Carga de Datos**\n",
    "- Cargar el dataset en un DataFrame de Pandas.\n",
    "- Inspeccionar las primeras filas y verificar información general de las variables.\n",
    "\n",
    "### **2. Preprocesamiento de Datos**\n",
    "- **Codificación de variables**: Transformar variables categóricas en numéricas si es necesario.\n",
    "- **Balanceo del dataset**: Verificar si la variable objetivo está balanceada y, si no lo está, aplicar técnicas de balanceo.\n",
    "- **Escalado de variables**: Normalizar o estandarizar las características si es necesario.\n",
    "- **Selección de características**: Determinar qué variables son más relevantes para el modelo.\n",
    "\n",
    "## **División del Dataset**\n",
    "El dataset se dividirá en:\n",
    "- **80%** para entrenamiento.\n",
    "- **20%** para prueba.\n",
    "- **Opcionalmente**, si se requiere, el conjunto de prueba se subdividirá en **10%** para validación y **90%** para evaluación.\n",
    "\n",
    "## **Construcción del Modelo**\n",
    "- Selección del modelo de clasificación adecuado.\n",
    "- Entrenamiento del modelo con los datos preparados.\n",
    "- Evaluación del desempeño del modelo con métricas adecuadas.\n",
    "\n",
    "## **Métrica de Desempeño**\n",
    "Para evaluar el modelo, se seleccionará una métrica de desempeño basada en la naturaleza del problema:\n",
    "- **Exactitud (Accuracy)**: Útil si el dataset está balanceado.\n",
    "- **F1-Score**: Adecuado en caso de un dataset desbalanceado.\n",
    "- **ROC-AUC**: Para evaluar la capacidad del modelo de distinguir entre clases positivas y negativas.\n",
    "\n",
    "Se justificará la elección de la métrica en función del análisis del dataset.\n",
    "\n",
    "## **Conclusión**\n",
    "Se analizarán los resultados obtenidos y se discutirán mejoras posibles en la clasificación de partidas de League of Legends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sjsj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = \"high_diamond_ranked_10min.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Seleccionar características y la variable objetivo\n",
    "target = \"blueWins\"\n",
    "features = [\"blueGoldDiff\", \"blueExperienceDiff\"]\n",
    "data = data[features + [target]]\n",
    "\n",
    "# Normalización de datos\n",
    "data[features] = (data[features] - data[features].mean()) / data[features].std()\n",
    "\n",
    "# División de datos\n",
    "X = data[features].values\n",
    "y = data[target].values\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
