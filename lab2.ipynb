{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Laboratorio 2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Julio García Salas - 22076**\n",
    "#### **Sofía García -22210**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Por qué el modelo de Naive Bayes se le considera “naive”?\n",
    "\n",
    "El modelo de Naive Bayes se considera \"naive\" (ingenuo) porque asume que todas las variables predictoras (features) son independientes entre sí, lo cual rara vez es cierto en la práctica. Esta suposición simplifica enormemente los cálculos, permitiendo una clasificación rápida y eficiente, aunque en muchos casos la independencia condicional no se cumple completamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machine (SVM)\n",
    "   - Explique la formulación matemática que se busca optimizar en Support Vector Machine.\n",
    "      El objetivo de **SVM** es encontrar un hiperplano que maximice la **margen** entre dos clases de datos.  \n",
    "      La formulación matemática de SVM se basa en minimizar la siguiente función objetivo:\n",
    "\n",
    "      $$\n",
    "      \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "      $$\n",
    "\n",
    "      ### Sujeto a las restricciones:\n",
    "\n",
    "      $$\n",
    "      y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "      $$\n",
    "\n",
    "      ### Donde:\n",
    "\n",
    "      **Vector de pesos:**  \n",
    "      $$\n",
    "      \\mathbf{w}\n",
    "      $$\n",
    "\n",
    "      **Sesgo (*bias*):**  \n",
    "      $$\n",
    "      b\n",
    "      $$\n",
    "\n",
    "      **Cada punto de datos de entrenamiento:**  \n",
    "      $$\n",
    "      \\mathbf{x}_i\n",
    "      $$\n",
    "\n",
    "      **Etiqueta de la clase:**  \n",
    "      $$\n",
    "      y_i \\in \\{+1, -1\\}\n",
    "      $$\n",
    "      \n",
    "\n",
    "      Si los datos no son perfectamente separables, se introduce un **término de relajación** con la función de pérdida **hinge loss**, lo que lleva a la formulación con variables de holgura $\\xi_i$:\n",
    "\n",
    "      $$\n",
    "      \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "      $$\n",
    "\n",
    "      ### Sujeto a:\n",
    "\n",
    "      $$\n",
    "      y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "      $$\n",
    "\n",
    "      Donde \\( C \\) es un hiperparámetro que controla la penalización por errores de clasificación.\n",
    "   - Responda: ¿cómo funciona el truco del Kernel para este modelo?  \n",
    "     _(Lo que se espera de esta pregunta es que puedan explicar en sus propias palabras la fórmula a la que llegamos que debemos optimizar de SVM en clase)._\n",
    "\n",
    "     El **truco del Kernel** permite transformar los datos a un espacio de mayor dimensión donde sean **linealmente separables**, sin necesidad de calcular explícitamente la transformación.  \n",
    "      En lugar de calcular la transformación $ \\phi(x) $, se usa una función **kernel** que evalúa el producto escalar en el espacio transformado:\n",
    "\n",
    "      $$\n",
    "      K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "      $$\n",
    "\n",
    "      ### Ejemplos de funciones de kernel comunes:\n",
    "\n",
    "      - **Lineal**:\n",
    "        $$\n",
    "        K(x_i, x_j) = x_i \\cdot x_j\n",
    "        $$\n",
    "\n",
    "      - **Polinómico**:\n",
    "        $$\n",
    "        K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "        $$\n",
    "\n",
    "      - **RBF (Radial Basis Function)**:\n",
    "        $$\n",
    "        K(x_i, x_j) = \\exp(-\\gamma \\| x_i - x_j \\|^2)\n",
    "        $$\n",
    "\n",
    "      Esto permite que **SVM maneje datos no linealmente separables** de manera eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest\n",
    "   - **a.** ¿Qué tipo de ensemble learning es este modelo?\n",
    "   \n",
    "   **Random Forest** es un modelo de **Bagging (Bootstrap Aggregating)**, que es un tipo de **ensemble learning basado en agregación**.  \n",
    "   - En **Bagging**, se generan múltiples modelos independientes (en este caso, árboles de decisión) entrenados en diferentes subconjuntos de los datos.  \n",
    "   - Luego, sus predicciones se combinan para mejorar la precisión y reducir la varianza del modelo.  \n",
    "   - En **clasificación**, se usa **votación mayoritaria**, y en **regresión**, se usa el **promedio de las predicciones**.\n",
    "\n",
    "   ---\n",
    "   - **b.** ¿Cuál es la idea general detrás de Random Forest?\n",
    "\n",
    "\n",
    "   La idea principal de **Random Forest** es entrenar **múltiples árboles de decisión** en subconjuntos aleatorios de los datos para reducir la varianza y mejorar la precisión.  \n",
    "El proceso general es el siguiente:\n",
    "\n",
    "1. **Bootstrap Sampling**: Se generan varios subconjuntos de los datos de entrenamiento mediante muestreo con reemplazo.\n",
    "2. **Entrenamiento de Árboles de Decisión**: Cada árbol se entrena en un subconjunto diferente.\n",
    "3. **Selección Aleatoria de Features**: En cada nodo del árbol, se selecciona aleatoriamente un subconjunto de variables para dividir los datos.\n",
    "4. **Combinación de Predicciones**:\n",
    "   - En **clasificación**, se usa **votación mayoritaria** entre los árboles.\n",
    "   - En **regresión**, se promedian las predicciones de todos los árboles.\n",
    "   Esto hace que **Random Forest sea más estable y generalice mejor** que un solo árbol de decisión, evitando el sobreajuste.\n",
    "\n",
    "---\n",
    "   - **c.** ¿Por qué se busca baja correlación entre los árboles de Random Forest?\n",
    "\n",
    "\n",
    "   Si los árboles dentro del bosque estuvieran altamente correlacionados, **cometerían los mismos errores**, y el ensemble no mejoraría la precisión.  \n",
    "Por eso, **se busca reducir la correlación** entre los árboles mediante:\n",
    "- **Bootstrap Sampling**, para que cada árbol vea diferentes datos de entrenamiento.\n",
    "- **Selección aleatoria de variables en cada nodo**, lo que obliga a los árboles a aprender características distintas.\n",
    "\n",
    "**Beneficios de una baja correlación entre árboles:**\n",
    "\n",
    "\n",
    "✅ **Mejora la generalización** → Reduce el sobreajuste.  \n",
    "✅ **Aumenta la estabilidad del modelo** → Si un árbol falla, los otros pueden corregirlo.  \n",
    "✅ **Reduce la varianza** → Los errores individuales de los árboles se cancelan entre sí.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clasificación de Mensajes como Spam o Ham usando Bayes/Laplace Smoothing**\n",
    "\n",
    "Este programa entrena un modelo basado en **Naive Bayes** con **Laplace Smoothing** para clasificar mensajes como **ham** (mensaje normal) o **spam** (correo no deseado).\n",
    "\n",
    "## **1. Requisitos del Programa**\n",
    "- Recibir como entrada un archivo llamado **\"entrenamiento.txt\"** (dataset de entrenamiento).\n",
    "- Usar **Naive Bayes** con **Laplace Smoothing** para clasificar mensajes.\n",
    "- Reportar métricas de desempeño del modelo.\n",
    "- Clasificar nuevos mensajes como **spam** o **ham**.\n",
    "- **Restricciones:**\n",
    "  - Se permite entrenar **solo un modelo por dataset** (no se pueden cargar múltiples archivos).\n",
    "  - Se debe **limpiar el dataset** eliminando caracteres especiales y normalizando mayúsculas/minúsculas.\n",
    "  - Cada línea del dataset representa **un mensaje** con su respectiva categoría.\n",
    "  - Se debe dividir el dataset en **training y test**.\n",
    "  - Se permite usar **librerías externas para la división de datos** (`sklearn`).\n",
    "  - ❌ **No se permite usar librerías externas para la construcción del modelo**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Cargar el Dataset y Preprocesarlo**\n",
    "Primero, **cargamos el dataset** y aplicamos preprocesamiento eliminando caracteres especiales y convirtiendo el texto a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etiqueta</th>\n",
       "      <th>mensaje</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  etiqueta                                            mensaje\n",
       "0      ham  go until jurong point crazy available only in ...\n",
       "1      ham                            ok lar joking wif u oni\n",
       "2     spam  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3      ham        u dun say so early hor u c already then say\n",
       "4      ham  nah i dont think he goes to usf he lives aroun..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def cargar_dataset(nombre_archivo):\n",
    "    datos = []\n",
    "    with open(nombre_archivo, \"r\", encoding=\"utf-8\") as file:\n",
    "        for linea in file:\n",
    "            etiqueta, mensaje = linea.strip().split(\"\\t\")  \n",
    "            mensaje = limpiar_texto(mensaje)  \n",
    "            datos.append((etiqueta, mensaje))\n",
    "    return pd.DataFrame(datos, columns=[\"etiqueta\", \"mensaje\"])\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", texto)  \n",
    "    texto = texto.lower().strip()  \n",
    "    return texto\n",
    "\n",
    "df = cargar_dataset(\"entrenamiento.txt\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Dividir en Training y Test**\n",
    "\n",
    "Se divide el dataset en 80% training y 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de entrenamiento: 4452\n",
      "Tamaño del dataset de prueba: 1113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"mensaje\"], df[\"etiqueta\"], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"Tamaño del dataset de entrenamiento: {len(X_train)}\")\n",
    "print(f\"Tamaño del dataset de prueba: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Implementar el modelo de Bayes con Laplace Smoothing sin uso de librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesSpamClassifier:\n",
    "    def __init__(self, laplace=1):\n",
    "        # Factor de laplace smoothing\n",
    "        self.laplace = laplace  \n",
    "        self.vocabulario = set()\n",
    "        self.palabras_spam = defaultdict(int)\n",
    "        self.palabras_ham = defaultdict(int)\n",
    "        self.total_spam = 0\n",
    "        self.total_ham = 0\n",
    "        self.num_spam = 0\n",
    "        self.num_ham = 0\n",
    "\n",
    "    def entrenar(self, X, y):\n",
    "        for mensaje, etiqueta in zip(X, y):\n",
    "            palabras = mensaje.split()\n",
    "            self.vocabulario.update(palabras)\n",
    "            if etiqueta == \"spam\":\n",
    "                self.num_spam += 1\n",
    "                for palabra in palabras:\n",
    "                    self.palabras_spam[palabra] += 1\n",
    "                    self.total_spam += 1\n",
    "            else:\n",
    "                self.num_ham += 1\n",
    "                for palabra in palabras:\n",
    "                    self.palabras_ham[palabra] += 1\n",
    "                    self.total_ham += 1\n",
    "\n",
    "    def predecir(self, mensaje):\n",
    "        palabras = mensaje.split()\n",
    "        vocab_size = len(self.vocabulario)\n",
    "\n",
    "        # Probabilidades iniciales\n",
    "        p_spam = np.log(self.num_spam / (self.num_spam + self.num_ham))\n",
    "        p_ham = np.log(self.num_ham / (self.num_spam + self.num_ham))\n",
    "\n",
    "        # Calcular probabilidades con Laplace Smoothing\n",
    "        for palabra in palabras:\n",
    "            p_spam += np.log((self.palabras_spam[palabra] + self.laplace) / (self.total_spam + vocab_size * self.laplace))\n",
    "            p_ham += np.log((self.palabras_ham[palabra] + self.laplace) / (self.total_ham + vocab_size * self.laplace))\n",
    "\n",
    "        return \"spam\" if p_spam > p_ham else \"ham\"\n",
    "\n",
    "\n",
    "modelo = NaiveBayesSpamClassifier()\n",
    "modelo.entrenar(X_train, y_train)\n",
    "print(\"Modelo entrenado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Evaluar Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar el modelo usamos:\n",
    "- **Precisión (Precision)**: Mide cuántos de los mensajes clasificados como spam realmente lo son.\n",
    "- **Recall (Sensibilidad)**: Mide cuántos de los mensajes spam fueron detectados correctamente.\n",
    "- **F1-score**: Es el balance entre precisión y recall.\n",
    "\n",
    "Estas métricas son importantes en clasificación de spam, ya que un **bajo recall** significa que el modelo no detecta suficiente spam, y un **bajo precision** significa que clasifica erróneamente mensajes ham como spam.ara evaluar el rendimiento del modelo, calculamos precisión, recall y f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       950\n",
      "        spam       0.90      0.96      0.93       163\n",
      "\n",
      "    accuracy                           0.98      1113\n",
      "   macro avg       0.95      0.97      0.96      1113\n",
      "weighted avg       0.98      0.98      0.98      1113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_pred = [modelo.predecir(mensaje) for mensaje in X_test]\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"ham\", \"spam\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
